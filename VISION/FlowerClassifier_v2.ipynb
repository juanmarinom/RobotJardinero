{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FlowerClassifier_v2(TF).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPuWkHa9HUbSMXNQ1V0DMUB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LLLehlxU44Ks","colab_type":"text"},"source":["#Versión 2.0\n","\n","Una vez probados los resultados con una red neuronal entrenada desde cero, se va a tratar de mejorar sensiblemente los resultados de la clasificación. El dataset del que se dispone es bastante pequeño para este tipo de entrenamientos (alrededor de 300 imágenes por clase), por lo que se hará uso de la técnica *Transfer Learning*.\n","\n","Primero, se instalan las dependecias necesarias y se importan los datos:\n"]},{"cell_type":"code","metadata":{"id":"2jFHiYvD58d8","colab_type":"code","colab":{}},"source":["!pip install tensorflow\n","!pip install keras==2.3.1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"COEbyS_U7f4p","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","!unzip '/content/drive/My Drive/Robot Jardinero/flowers.zip' -d '/content/flower_data'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mdHf3_CU6IqN","colab_type":"text"},"source":["Después, se importa el VGG16, una red neuronal convolucional propuesta por K. Simonyan y A. Zisserman. Este modelo consigue un 92.7% de precisión en ImageNet, un dataset con más de 14 millones de imágenes pertenecientes a 1000 clases."]},{"cell_type":"code","metadata":{"id":"5sdtFhaL4yVz","colab_type":"code","outputId":"b1ed964a-59ce-40f6-8eb7-2502f9da8ba5","executionInfo":{"status":"ok","timestamp":1587312524889,"user_tz":-120,"elapsed":792289,"user":{"displayName":"J SR","photoUrl":"","userId":"03743565986024427624"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import numpy as np\n","from keras.applications import VGG16\n","from keras.applications.vgg16 import preprocess_input\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten\n","from keras.layers import Conv2D, MaxPooling2D, Dropout, BatchNormalization\n","from keras.optimizers import Adam\n","\n","train_datagen=ImageDataGenerator(rescale=1./255.,\n","                                 #rotation_range=40,\n","                                 #width_shift_range=0.2,\n","                                 #height_shift_range=0.2,\n","                                 #shear_range=0.2,\n","                                 #zoom_range=0.2,\n","                                 #horizontal_flip=True,\n","                                 #fill_mode=\"nearest\",\n","                                 validation_split=0.2,\n","                                 preprocessing_function=preprocess_input)\n","\n","train_generator = train_datagen.flow_from_directory('/content/flower_data/flowers',\n","                                                    target_size=(150,150), #to meet VGG requirements\n","                                                    color_mode='rgb',\n","                                                    batch_size=32,\n","                                                    class_mode='categorical',\n","                                                    subset='training')\n","validation_generator = train_datagen.flow_from_directory('/content/flower_data/flowers',\n","                                                         target_size=(150,150),\n","                                                         color_mode='rgb',\n","                                                         batch_size=32,\n","                                                         class_mode='categorical',\n","                                                         subset='validation')\n","\n","#Como base, tomaremos el VGG16, con casi 15 millones de parámetros entrenables\n","base_model=VGG16(weights=\"imagenet\",\n","                 include_top=False, \n","                 input_shape=(150,150,3))\n","\n","\n","#Se añade una última capa (Fully Connected), que será la que se entrenará con las imágenes\n","modeloPropio = Sequential()\n","modeloPropio.add(base_model)\n","base_model.summary()\n","\n","modeloPropio.add(Flatten())\n","modeloPropio.add(Dense(units=4096,activation=\"relu\"))\n","#modeloPropio.add(BatchNormalization())\n","modeloPropio.add(Dense(units=2048,activation=\"relu\"))\n","#modeloPropio.add(BatchNormalization())\n","modeloPropio.add(Dense(units=1024,activation=\"relu\"))\n","modeloPropio.add(Dense(units=64,activation=\"relu\"))\n","modeloPropio.add(Dense(5, activation=\"softmax\"))\n","\n","modeloPropio.summary()\n","\n","from keras.callbacks import ReduceLROnPlateau\n","red_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)\n","\n","#Establecemos que el modelo de VGG16 ya está entrenado, por lo que se deshabilita como parte del entrenamiento\n","base_model.trainable=False\n","\n","#custom optimizer\n","sgd = optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.9, nesterov=True)\n","\n","modeloPropio.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001), metrics=[\"accuracy\"])\n","\n","#step size\n","step_size_train = train_generator.n//train_generator.batch_size\n","step_size_valid = validation_generator.n//validation_generator.batch_size\n","\n","\n","\n","#fit the model\n","history = modeloPropio.fit_generator(train_generator,\n","                                     steps_per_epoch=step_size_train,\n","                                     epochs=30,\n","                                     validation_data=validation_generator,\n","                                     validation_steps=step_size_valid,\n","                                     shuffle = True)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Found 3462 images belonging to 5 classes.\n","Found 861 images belonging to 5 classes.\n","Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_9 (InputLayer)         (None, 150, 150, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"sequential_9\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Model)                (None, 4, 4, 512)         14714688  \n","_________________________________________________________________\n","flatten_9 (Flatten)          (None, 8192)              0         \n","_________________________________________________________________\n","dense_33 (Dense)             (None, 4096)              33558528  \n","_________________________________________________________________\n","dense_34 (Dense)             (None, 2048)              8390656   \n","_________________________________________________________________\n","dense_35 (Dense)             (None, 1024)              2098176   \n","_________________________________________________________________\n","dense_36 (Dense)             (None, 64)                65600     \n","_________________________________________________________________\n","dense_37 (Dense)             (None, 5)                 325       \n","=================================================================\n","Total params: 58,827,973\n","Trainable params: 58,827,973\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/30\n","108/108 [==============================] - 27s 251ms/step - loss: 1.0969 - accuracy: 0.6137 - val_loss: 0.6589 - val_accuracy: 0.7608\n","Epoch 2/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.4810 - accuracy: 0.8292 - val_loss: 0.6797 - val_accuracy: 0.7081\n","Epoch 3/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.3224 - accuracy: 0.8869 - val_loss: 0.2916 - val_accuracy: 0.7841\n","Epoch 4/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.2258 - accuracy: 0.9149 - val_loss: 0.9128 - val_accuracy: 0.8022\n","Epoch 5/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.1546 - accuracy: 0.9478 - val_loss: 0.8595 - val_accuracy: 0.8106\n","Epoch 6/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.1092 - accuracy: 0.9624 - val_loss: 0.9400 - val_accuracy: 0.7937\n","Epoch 7/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.1077 - accuracy: 0.9636 - val_loss: 1.1069 - val_accuracy: 0.8106\n","Epoch 8/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.0668 - accuracy: 0.9778 - val_loss: 0.8611 - val_accuracy: 0.7780\n","Epoch 9/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0875 - accuracy: 0.9714 - val_loss: 1.5238 - val_accuracy: 0.7419\n","Epoch 10/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.1112 - accuracy: 0.9627 - val_loss: 0.4434 - val_accuracy: 0.8408\n","Epoch 11/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0845 - accuracy: 0.9723 - val_loss: 1.1065 - val_accuracy: 0.7720\n","Epoch 12/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0665 - accuracy: 0.9790 - val_loss: 1.9169 - val_accuracy: 0.7877\n","Epoch 13/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.0430 - accuracy: 0.9883 - val_loss: 2.1741 - val_accuracy: 0.7961\n","Epoch 14/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.0260 - accuracy: 0.9918 - val_loss: 1.3850 - val_accuracy: 0.7684\n","Epoch 15/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0319 - accuracy: 0.9883 - val_loss: 0.3764 - val_accuracy: 0.7949\n","Epoch 16/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0474 - accuracy: 0.9840 - val_loss: 1.8235 - val_accuracy: 0.7346\n","Epoch 17/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0302 - accuracy: 0.9901 - val_loss: 0.3094 - val_accuracy: 0.7479\n","Epoch 18/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.0297 - accuracy: 0.9907 - val_loss: 0.3919 - val_accuracy: 0.7720\n","Epoch 19/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0989 - accuracy: 0.9709 - val_loss: 1.5940 - val_accuracy: 0.7636\n","Epoch 20/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0729 - accuracy: 0.9784 - val_loss: 0.7006 - val_accuracy: 0.8034\n","Epoch 21/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0355 - accuracy: 0.9910 - val_loss: 1.5065 - val_accuracy: 0.7455\n","Epoch 22/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.0476 - accuracy: 0.9855 - val_loss: 2.2406 - val_accuracy: 0.7913\n","Epoch 23/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0391 - accuracy: 0.9900 - val_loss: 1.8867 - val_accuracy: 0.7805\n","Epoch 24/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0303 - accuracy: 0.9930 - val_loss: 0.8612 - val_accuracy: 0.7998\n","Epoch 25/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0556 - accuracy: 0.9843 - val_loss: 2.2625 - val_accuracy: 0.7768\n","Epoch 26/30\n","108/108 [==============================] - 26s 243ms/step - loss: 0.0864 - accuracy: 0.9813 - val_loss: 0.8762 - val_accuracy: 0.7587\n","Epoch 27/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.0398 - accuracy: 0.9886 - val_loss: 0.6255 - val_accuracy: 0.7732\n","Epoch 28/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.0320 - accuracy: 0.9919 - val_loss: 1.0304 - val_accuracy: 0.7861\n","Epoch 29/30\n","108/108 [==============================] - 26s 242ms/step - loss: 0.0329 - accuracy: 0.9909 - val_loss: 1.0521 - val_accuracy: 0.8034\n","Epoch 30/30\n","108/108 [==============================] - 26s 244ms/step - loss: 0.0326 - accuracy: 0.9936 - val_loss: 1.5885 - val_accuracy: 0.7998\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k-MsZ0hENgF1","colab_type":"code","outputId":"21d98790-31b5-4d03-e09d-a32514212f0e","executionInfo":{"status":"error","timestamp":1587323638460,"user_tz":-120,"elapsed":2268,"user":{"displayName":"J SR","photoUrl":"","userId":"03743565986024427624"}},"colab":{"base_uri":"https://localhost:8080/","height":246}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Plot training & validation accuracy values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b3ce6f7a8221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot training & validation accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeloPropio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeloPropio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'modeloPropio' is not defined"]}]},{"cell_type":"code","metadata":{"id":"MkpSH206M1ka","colab_type":"code","colab":{}},"source":["import os\n","# Guardar el Modelo\n","modeloPropio.save('/content/drive/My Drive/TF_85.h5')\n"],"execution_count":0,"outputs":[]}]}